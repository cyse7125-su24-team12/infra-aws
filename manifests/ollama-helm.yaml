ollama: 
  gpu: 
    enabled: true
  models: 
  - llama3.1:8b

persistentVolume:
  enabled: true
  size: "40Gi"

resource: 
  limits:
    nvidia.com/gpu: 1
tolerations: 
  - key: "gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
